# -*- coding: utf-8 -*-
"""ISL

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yk12G_1DbmxMUaEhVFVSUyFVEZ458_L9

# **Data Preprocessing:**

# *Loading the dataset:*
"""

from google.colab import drive
import os
import cv2
import glob
import matplotlib.pyplot as plt
from collections import Counter
from IPython.display import display, Image

# Mount Google Drive
drive.mount('/content/drive')

# Define dataset path (Update this to your actual dataset path)
dataset_path = '/content/drive/MyDrive/FINAL ISL/isl'

# Supported formats
image_extensions = ['.jpg', '.jpeg', '.png']
video_extensions = ['.mp4', '.avi', '.mov']

# Lists to store file paths
image_files = []
video_files = []
labels = []

# Traverse dataset folder
for root, _, files in os.walk(dataset_path):
    for file in files:
        file_path = os.path.join(root, file)
        label = os.path.basename(root)  # Assuming each sign has its own folder
        if any(file.lower().endswith(ext) for ext in image_extensions):
            image_files.append(file_path)
            labels.append(label)
        elif any(file.lower().endswith(ext) for ext in video_extensions):
            video_files.append(file_path)
            labels.append(label)

# Display dataset summary
print(f"Total Images: {len(image_files)}")
print(f"Total Videos: {len(video_files)}")

# Show a sample image (if available)
if image_files:
    display(Image(filename=image_files[0]))

# Show a sample video frame (if available)
if video_files:
    cap = cv2.VideoCapture(video_files[0])
    ret, frame = cap.read()
    cap.release()
    if ret:
        cv2.imwrite('sample_frame.jpg', frame)

"""# *Plot dataset distribution:*"""

label_counts = Counter(labels)
plt.figure(figsize=(12, 6))
plt.bar(label_counts.keys(), label_counts.values(), color='skyblue')
plt.xticks(rotation=45)
plt.xlabel("Signs")
plt.ylabel("Count")
plt.title("Dataset Distribution")
plt.show()

"""# *Video Preprocessing:*"""

import cv2
import os
from tqdm import tqdm
import numpy as np

# Paths
video_dataset_path = "/content/drive/MyDrive/FINAL ISL/isl"  # Raw videos
preprocessed_path = "/content/drive/MyDrive/FINAL ISL/preprocessed_isl"  # Folder to store extracted frames
os.makedirs(preprocessed_path, exist_ok=True)

# Video classes (labels with video data)
video_classes = {'H', 'J', 'Y', '6', '9', '10'}  # Only these have videos

# Set parameters
num_frames = 8  # Extract exactly 8 frames per video
image_size = (224, 224)  # Resize frames

# Process each label folder (H, J, Y, 6, 9, 10)
for label in video_classes:
    label_folder = os.path.join(video_dataset_path, label)
    output_label_folder = os.path.join(preprocessed_path, label)
    os.makedirs(output_label_folder, exist_ok=True)

    # Get video files in this label folder
    video_files = [f for f in os.listdir(label_folder) if f.endswith(('.mp4', '.avi', '.mov'))]

    if not video_files:
        print(f"âš  No videos found for label '{label}'. Skipping...")
        continue

    print(f"\nðŸŽ¥ Processing videos for label: {label} | Total Videos: {len(video_files)}")

    for video_file in tqdm(video_files, desc=f"Extracting {label} frames"):
        video_path = os.path.join(label_folder, video_file)

        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"âš  Error: Could not open video '{video_file}' in label '{label}'")
            continue  # Skip this video

        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Total frames in the video
        if frame_count < num_frames:
            print(f"âš  Warning: '{video_file}' has only {frame_count} frames. Extracting available frames.")
            frame_indices = np.linspace(0, frame_count - 1, frame_count, dtype=int)  # Take all available
        else:
            frame_indices = np.linspace(0, frame_count - 1, num_frames, dtype=int)  # Select 8 evenly spaced frames

        saved_frames = 0
        for i in range(frame_count):
            success, frame = cap.read()
            if not success:
                break  # No more frames to read

            if i in frame_indices:
                frame = cv2.resize(frame, image_size)
                frame_filename = os.path.join(output_label_folder, f'{os.path.splitext(video_file)[0]}_frame_{saved_frames:02d}.jpg')
                cv2.imwrite(frame_filename, frame)
                saved_frames += 1

        cap.release()
        print(f"âœ… Extracted {saved_frames} frames from '{video_file}'")

    print(f"âœ… Done processing videos for label: {label}")

print(f"\nðŸŽ‰ Video frame extraction complete. Frames saved in {preprocessed_path}")

"""# *Image Preprocessing:*"""

import cv2
import os
import numpy as np

input_path = "/content/drive/MyDrive/FINAL ISL/isl"
output_path = "/content/drive/MyDrive/FINAL ISL/preprocessed_isl"
image_size = (224, 224)


os.makedirs(output_path, exist_ok=True)

def process_image(img_path, output_dir):
    img = cv2.imread(img_path)
    if img is not None:
        img = cv2.resize(img, image_size)
        img = img / 255.0  # Normalize to [0,1]
        save_path = os.path.join(output_dir, os.path.basename(img_path))
        cv2.imwrite(save_path, (img * 255).astype(np.uint8))


for label in os.listdir(input_path):
    label_folder = os.path.join(input_path, label)
    output_label_folder = os.path.join(output_path, label)
    os.makedirs(output_label_folder, exist_ok=True)

    for file in os.listdir(label_folder):
        if file.endswith(('.jpg', '.png', '.jpeg')):
            img_path = os.path.join(label_folder, file)
            process_image(img_path, output_label_folder)

print(f"Image preprocessing complete. Saved in {output_path}")

"""# *Dataset distribution:*"""

import os
import matplotlib.pyplot as plt
from collections import Counter

# Path to preprocessed dataset
preprocessed_path = "/content/drive/MyDrive/FINAL ISL/preprocessed_isl"

# Count images in each label folder
label_counts = {}
for label in os.listdir(preprocessed_path):
    label_folder = os.path.join(preprocessed_path, label)
    if os.path.isdir(label_folder):  # Ensure it's a folder
        label_counts[label] = len([f for f in os.listdir(label_folder) if f.endswith('.jpg')])

# Plot dataset distribution
plt.figure(figsize=(12, 6))
plt.bar(label_counts.keys(), label_counts.values(), color='skyblue')
plt.xticks(rotation=45)
plt.xlabel("Signs")
plt.ylabel("Count")
plt.title("Preprocessed Dataset Distribution")
plt.show()

"""# *Data Augmentation:*"""

import os
import cv2
import numpy as np
import albumentations as A
import shutil

# Paths
preprocessed_path = "/content/drive/MyDrive/FINAL ISL/preprocessed_isl"  # Source folder
augmented_path = "/content/drive/MyDrive/FINAL ISL/augmented_isl"  # Destination folder
os.makedirs(augmented_path, exist_ok=True)

# Target number of images per label
TARGET_COUNT = 100

# Labels that contain only images (excluding video frame labels)
image_labels = {str(i) for i in range(0, 11)} | {  # Numbers 0-10
    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'I', 'K', 'L', 'M', 'N', 'O',
    'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Z','1', '2', '3', '4', '5', '7', '8'  # Letters (excluding H, J, Y)
}

# Augmentation pipeline
transform = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(p=0.3),
    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=0.7),
    A.GaussianBlur(blur_limit=(3, 7), p=0.3),
    A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=0.5)
])

# Process each label
for label in image_labels:
    input_folder = os.path.join(preprocessed_path, label)
    output_folder = os.path.join(augmented_path, label)

    # Check if the label folder exists in preprocessed data
    if not os.path.exists(input_folder):
        print(f"âš ï¸ Warning: Folder {input_folder} not found. Skipping...")
        continue

    os.makedirs(output_folder, exist_ok=True)

    # Get all images in label folder (include .jpg and .jpeg)
    image_files = sorted([f for f in os.listdir(input_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])
    image_paths = [os.path.join(input_folder, f) for f in image_files]

    # Ensure no images are missed
    if len(image_files) == 0:
        print(f"âš ï¸ Warning: No images found in {input_folder}. Skipping...")
        continue

    # Copy original images to augmented folder
    for img_path in image_paths:
        shutil.copy(img_path, output_folder)

    # Count existing images and determine how many more are needed
    num_existing = len(image_paths)
    num_to_generate = TARGET_COUNT - num_existing

    if num_to_generate > 0:
        print(f"ðŸ“¢ Augmenting {num_to_generate} images for label {label}...")

        # Augment images until we reach TARGET_COUNT
        augmented_images = 0
        index = 0

        while augmented_images < num_to_generate:
            img_path = image_paths[index % num_existing]
            img = cv2.imread(img_path)

            if img is None:
                print(f"âŒ Error reading image: {img_path}")
                index += 1
                continue

            img = img.astype(np.float32) / 255.0  # Normalize
            augmented = transform(image=img)['image']

            aug_save_path = os.path.join(output_folder, f'aug_{augmented_images + num_existing:04d}.jpg')
            cv2.imwrite(aug_save_path, (augmented * 255).astype(np.uint8))

            augmented_images += 1
            index += 1

    print(f"âœ… {label}: {TARGET_COUNT} images saved in {output_folder}")

print("\nðŸŽ‰ Image augmentation complete! All labels now have 100 images.")

"""# *Final preprocessed data:*"""

import os
import shutil

# Paths
preprocessed_path = "/content/drive/MyDrive/FINAL ISL/preprocessed_isl"  # Source folder
augmented_path = "/content/drive/MyDrive/FINAL ISL/augmented_isl"  # Destination folder

# Ensure the destination folder exists
os.makedirs(augmented_path, exist_ok=True)

# Get all labels (folders) from preprocessed dataset
labels = [label for label in os.listdir(preprocessed_path) if os.path.isdir(os.path.join(preprocessed_path, label))]

# Copy all labels and their contents
for label in labels:
    input_folder = os.path.join(preprocessed_path, label)
    output_folder = os.path.join(augmented_path, label)

    os.makedirs(output_folder, exist_ok=True)

    # Copy all files from the preprocessed folder to the augmented folder
    for file in os.listdir(input_folder):
        src_file = os.path.join(input_folder, file)
        dst_file = os.path.join(output_folder, file)
        shutil.copy(src_file, dst_file)

    print(f"âœ… Copied all data for label '{label}' to {output_folder}")

print("\nðŸŽ‰ All preprocessed data successfully copied to the augmented folder!")

"""# *Augmented Dataset Distribution:*"""

import os
import matplotlib.pyplot as plt
from collections import Counter

# Path to preprocessed dataset
preprocessed_path = "/content/drive/MyDrive/FINAL ISL/augmented_isl"

# Count images in each label folder
label_counts = {}
for label in os.listdir(preprocessed_path):
    label_folder = os.path.join(preprocessed_path, label)
    if os.path.isdir(label_folder):  # Ensure it's a folder
        label_counts[label] = len([f for f in os.listdir(label_folder) if f.endswith('.jpg')])

# Plot dataset distribution
plt.figure(figsize=(12, 6))
plt.bar(label_counts.keys(), label_counts.values(), color='skyblue')
plt.xticks(rotation=45)
plt.xlabel("Signs")
plt.ylabel("Count")
plt.title("augmented Dataset Distribution")
plt.show()

!pip install mediapipe

"""# *One-Hand keypoint detection:*"""

import cv2
import os
import mediapipe as mp

# Paths
augmented_path = "/content/drive/MyDrive/FINAL ISL/augmented_isl"  # Augmented dataset
keypoints_path = "/content/drive/MyDrive/FINAL ISL/keypoints_isl"  # Folder to save hand keypoint images
os.makedirs(keypoints_path, exist_ok=True)

# Initialize MediaPipe Hands for detecting multiple hands
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)

# Process each label
for label in os.listdir(augmented_path):
    label_folder = os.path.join(augmented_path, label)
    output_folder = os.path.join(keypoints_path, label)
    os.makedirs(output_folder, exist_ok=True)

    print(f"Processing label: {label}")

    for file in os.listdir(label_folder):
        if not file.lower().endswith(('.jpg', '.jpeg', '.png')):  # Only process images
            continue

        img_path = os.path.join(label_folder, file)
        img = cv2.imread(img_path)
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # Detect hands
        results = hands.process(img_rgb)

        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                mp_drawing.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)

            # Save the image with hand keypoints
            save_path = os.path.join(output_folder, file)
            cv2.imwrite(save_path, img)

    print(f"âœ… Keypoint images saved for label {label}")

print("\nðŸŽ‰ Hand landmark images saved successfully in", keypoints_path)

"""# *Two Hand Keypoint Detection:*"""

import cv2
import os
import mediapipe as mp

# Paths
augmented_path = "/content/drive/MyDrive/FINAL ISL/augmented_isl"  # Source folder
keypoints_path = "/content/drive/MyDrive/FINAL ISL/hand_keypoints_isl"  # Destination folder
os.makedirs(keypoints_path, exist_ok=True)

# Initialize MediaPipe Hands
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

# Process each label folder
for label in os.listdir(augmented_path):
    label_folder = os.path.join(augmented_path, label)
    output_folder = os.path.join(keypoints_path, label)
    os.makedirs(output_folder, exist_ok=True)

    for file in os.listdir(label_folder):
        if file.lower().endswith(('.jpg', '.jpeg', '.png')):
            img_path = os.path.join(label_folder, file)
            output_img_path = os.path.join(output_folder, file)

            # Read image
            image = cv2.imread(img_path)
            if image is None:
                print(f"Error loading image: {img_path}")
                continue

            # Convert image to RGB
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # Detect hands
            with mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5) as hands:
                results = hands.process(rgb_image)

                if results.multi_hand_landmarks:
                    for hand_landmarks in results.multi_hand_landmarks:
                        mp_drawing.draw_landmarks(
                            image, hand_landmarks, mp_hands.HAND_CONNECTIONS,
                            mp_drawing_styles.get_default_hand_landmarks_style(),
                            mp_drawing_styles.get_default_hand_connections_style()
                        )

            # Save the output image
            cv2.imwrite(output_img_path, image)

    print(f"âœ… Processed hand keypoints for label {label}")

print("ðŸŽ‰ Hand keypoint detection complete! Images saved in", keypoints_path)

"""# **Model Building:**

1. Verify Data Quality & Balance
Check if all labels have exactly 100 images in the augmented dataset (both images and frames).
Visualize the dataset distribution using a bar chart.
Ensure that images are correctly labeled and contain clear hand signs.
2. Hand Keypoint Extraction (2 Hands)
You need to extract hand keypoints from the augmented dataset using MediaPipe (2-hand detection).
Save keypoint images in a new folder (e.g., keypoints_isl).
Ensure the correct hand(s) are detected in every image.
3. Convert Keypoint Data for CNN Input
Convert landmark images into a format suitable for CNN (you can use the raw keypoint images directly).
Optionally, normalize pixel values between 0 and 1.
Resize images to 224x224 (or a size suitable for your CNN).
4. Split Dataset into Train, Validation, and Test Sets
Training Set (70%) â€“ Used to train the model.
Validation Set (15%) â€“ Used to tune hyperparameters.
Test Set (15%) â€“ Used to evaluate final performance.
5. Data Augmentation (Optional for Training)
Apply real-time augmentation (e.g., rotation, flipping) to improve model generalization.
6. Choose a CNN Architecture
Pretrained Models (e.g., MobileNetV2, ResNet50, EfficientNet)
Custom CNN (if you prefer to design a smaller, task-specific model)
7. Prepare Model Training Pipeline
Define loss function & optimizer (e.g., categorical cross-entropy, Adam).
Choose evaluation metrics (e.g., accuracy, F1-score).
Implement data loaders for efficient training.

# *Load and Preprocess Data*
"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define dataset path
dataset_path = "/content/drive/MyDrive/FINAL ISL/hand_keypoints_isl"

# Only rescale images (no resizing needed)
datagen = ImageDataGenerator(rescale=1.0/255, validation_split=0.2)

train_generator = datagen.flow_from_directory(
    dataset_path,
    target_size=(224, 224),  # Keep the same size as the dataset
    batch_size=32,
    class_mode='categorical',  # Change to 'binary' if only 2 classes
    subset='training'
)

val_generator = datagen.flow_from_directory(
    dataset_path,
    target_size=(224, 224),  # Keep consistent size
    batch_size=32,
    class_mode='categorical',
    subset='validation'
)

"""# *Build the CNN Model:*"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# Define CNN model
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(train_generator.num_classes, activation='softmax')  # Adjust for number of classes
])

# Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Model summary
model.summary()

"""# *BatchNormalization:*"""

from tensorflow.keras.layers import GlobalAveragePooling2D, BatchNormalization

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),

    GlobalAveragePooling2D(),
    BatchNormalization(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(train_generator.num_classes, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

"""# *EarlyStopping:*"""

from tensorflow.keras.callbacks import EarlyStopping

# Stop training when validation loss stops improving
early_stopping = EarlyStopping(
    monitor='val_loss',  # Track validation loss
    patience=5,  # Stop if no improvement for 5 epochs
    restore_best_weights=True
)

# Train model with EarlyStopping
epochs = 50  # Set a high number, but training will stop early if needed

history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=epochs,
    callbacks=[early_stopping]
)

"""# *Predict the results:*"""

import numpy as np
from tensorflow.keras.preprocessing import image
import os

# Update img_path to a valid image file path within the directory
img_path = "/content/drive/MyDrive/FINAL ISL/hand_keypoints_isl/1/WhatsApp Image 2025-02-03 at 4.47.24 PM.jpeg"


# Load the image for prediction
img = image.load_img(img_path, target_size=(224, 224))
img_array = image.img_to_array(img) / 255.0
img_array = np.expand_dims(img_array, axis=0)

# Predict class
predictions = model.predict(img_array)
predicted_class = np.argmax(predictions)
confidence = np.max(predictions)

print(f"Predicted Class: {predicted_class}, Confidence: {confidence * 100:.2f}%")

"""# *Evaluation of the model using confusion matrix:*"""

import seaborn as sns
import tensorflow as tf
from sklearn.metrics import confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

# Get predictions on the validation set
y_true = val_generator.classes
y_pred = model.predict(val_generator)
y_pred_classes = np.argmax(y_pred, axis=1)

# Compute confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=val_generator.class_indices, yticklabels=val_generator.class_indices)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

"""# *Key Points Extraction:*"""

import cv2
import os
import numpy as np
import mediapipe as mp

# Paths
dataset_path = "/content/drive/MyDrive/FINAL ISL/augmented_isl"
labels = sorted(os.listdir(dataset_path))  # Get all sign labels

# Initialize MediaPipe Hands
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)

# Function to extract keypoints
def extract_keypoints(image):
    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = hands.process(img_rgb)
    keypoints = []

    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            for lm in hand_landmarks.landmark:
                keypoints.extend([lm.x, lm.y, lm.z])

    # Return keypoints or zero array if no hand detected
    # Ensure consistent shape: If no keypoints, pad with zeros to match the expected shape (126,)
    expected_shape = (42 * 3)  # 42 landmarks * 3 (x, y, z)
    if not keypoints:
        keypoints = np.zeros(expected_shape)
    else:
        keypoints = np.array(keypoints)
        # If keypoints have a different length, pad or truncate:
        if keypoints.shape[0] != expected_shape:
            if keypoints.shape[0] < expected_shape:
                keypoints = np.pad(keypoints, (0, expected_shape - keypoints.shape[0]), 'constant')
            else:
                keypoints = keypoints[:expected_shape]

    return keypoints

# Prepare dataset
X, y = [], []
for label_idx, label in enumerate(labels):
    label_folder = os.path.join(dataset_path, label)
    print(f"Processing {label}...")

    for file in os.listdir(label_folder):
        if file.lower().endswith(('.jpg', '.jpeg', '.png')):  # Process only images
            img_path = os.path.join(label_folder, file)
            img = cv2.imread(img_path)
            keypoints = extract_keypoints(img)

            X.append(keypoints)
            y.append(label_idx)

# Convert to NumPy arrays & save
X = np.array(X)
y = np.array(y)
np.save("X.npy", X)
np.save("y.npy", y)

print("âœ… Keypoints extracted and saved!")

"""# *CNN model for image:*"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout, BatchNormalization
from sklearn.model_selection import train_test_split

# Load dataset
X_static = np.load("X.npy")
y_static = np.load("y.npy")

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_static, y_static, test_size=0.2, random_state=42, stratify=y_static)

# Reshape for CNN (adding channel dimension)
X_train = X_train.reshape(-1, X_train.shape[1], 1)
X_test = X_test.reshape(-1, X_test.shape[1], 1)

# CNN model
cnn_model = Sequential([
    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),
    BatchNormalization(),
    Conv1D(128, kernel_size=3, activation='relu'),
    BatchNormalization(),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(len(set(y_static)), activation='softmax')  # Adjust based on number of classes
])

# Compile model
cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train model
cnn_model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))

# Evaluate model
loss, accuracy = cnn_model.evaluate(X_test, y_test)
print(f"âœ… CNN Model Accuracy: {accuracy * 100:.2f}%")

"""# *CNN model for frames:*"""

import cv2
import os
import numpy as np

# Paths
dataset_path = "/content/drive/MyDrive/FINAL ISL/isl"
from google.colab import drive
drive.mount('/content/drive')
# Only include these labels
selected_labels = {"6", "9", "10", "H", "J", "Y"}

# Filter dataset folders based on selected labels
labels = sorted([label for label in os.listdir(dataset_path) if label in selected_labels])

sequence_length = 30  # Fixed number of frames per video
frame_size = (128, 128)  # Resize frames to 128x128

X_dynamic = []
y_dynamic = []

for label_idx, label in enumerate(labels):
    label_folder = os.path.join(dataset_path, label)
    print(f"Processing video label: {label}...")

    for video in os.listdir(label_folder):
        frames = []
        cap = cv2.VideoCapture(os.path.join(label_folder, video))

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            frame = cv2.resize(frame, frame_size)  # Resize frame
            frame = frame / 255.0  # Normalize pixel values (0-1)
            frames.append(frame)

            if len(frames) == sequence_length:
                X_dynamic.append(frames)
                y_dynamic.append(label_idx)
                frames = []  # Reset buffer for next sequence

        cap.release()

# Convert to NumPy arrays
X_dynamic = np.array(X_dynamic)
y_dynamic = np.array(y_dynamic)

# Save processed dataset
np.save("X_dynamic.npy", X_dynamic)
np.save("y_dynamic.npy", y_dynamic)

print("âœ… Videos for labels 6, 9, 10, H, J, Y converted to frame sequences and saved successfully!")

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LSTM, TimeDistributed, Input
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

# Load dataset
X = np.load("X_dynamic.npy")
y = np.load("y_dynamic.npy")

# One-hot encode labels
num_classes = len(np.unique(y))
y = to_categorical(y, num_classes)

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define model architecture
model = Sequential([
    Input(shape=(30, 128, 128, 3)),  # 30 frames per sequence, each frame is 128x128 with 3 channels
    TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same')),
    TimeDistributed(MaxPooling2D((2, 2))),
    TimeDistributed(Flatten()),
    LSTM(64, return_sequences=False),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

# Compile model
model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model with a smaller batch size
model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=4, epochs=20, verbose=1) # Reduced batch size to 4

# Save the trained model
model.save("sign_language_model.h5")

print("âœ… Model trained and saved successfully!")

import numpy as np
from tensorflow.keras.models import load_model
from tensorflow.keras.utils import to_categorical

# Load dataset
X_test = np.load("X_dynamic.npy")
y_test = np.load("y_dynamic.npy")

# Ensure y_test is one-hot encoded
num_classes = len(np.unique(y_test))  # Number of unique labels
y_test = to_categorical(y_test, num_classes)

# Load the trained model
model = load_model("sign_language_model.h5")

# Compile the model (fix for missing metrics warning)
model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])

# Evaluate model on test set
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)
print(f"âœ… Test Accuracy: {test_accuracy * 100:.2f}%")

# Make predictions
y_pred = model.predict(X_test)

# Convert predictions from one-hot encoding to label indices
y_pred_labels = np.argmax(y_pred, axis=1)
y_test_labels = np.argmax(y_test, axis=1)

# Display some sample predictions
for i in range(5):  # Show 5 test samples
    print(f"Actual: {y_test_labels[i]}, Predicted: {y_pred_labels[i]}")

import matplotlib.pyplot as plt
import cv2
import numpy as np

# Define class labels
class_labels = ['6', '9', '10', 'H', 'J', 'Y']  # Modify based on your dataset

# Randomly select different test samples
num_samples = 5  # Number of images to display
indices = np.random.choice(len(X_test), num_samples, replace=False)  # Select random indices

plt.figure(figsize=(10, 10))

for i, idx in enumerate(indices):
    plt.subplot(1, num_samples, i + 1)

    # Extract first frame of the video sequence for visualization
    img = X_test[idx][0]  # First frame of the sequence
    img = (img * 255).astype(np.uint8)  # Convert back to original pixel range

    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB
    plt.axis("off")

    # Convert one-hot encoded labels back to class indices
    actual_label = class_labels[np.argmax(y_test[idx])]
    predicted_label = class_labels[np.argmax(y_pred[idx])]

    plt.title(f"Actual: {actual_label}\nPred: {predicted_label}")

plt.show()

from google.colab import drive
# Mount Google Drive
drive.mount('/content/drive')

"""# *VGG16 Model:*"""

import os
import cv2
import numpy as np
import mediapipe as mp
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Model
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# Paths
dataset_path = "/content/drive/MyDrive/FINAL ISL/isl"

# MediaPipe Hand Detection
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)

# Collect all available labels
all_labels = sorted([label for label in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, label))])
print("âœ… Available Classes:", all_labels)

# Encode labels
encoder = LabelEncoder()
encoder.fit(all_labels)  # Learn all labels

# Load dataset
X, y = [], []
for label in all_labels:
    label_folder = os.path.join(dataset_path, label)
    for file in os.listdir(label_folder):
        if file.endswith(('.jpg', '.png', '.jpeg')):  # Process images
            img_path = os.path.join(label_folder, file)
            img = cv2.imread(img_path)
            img = cv2.resize(img, (224, 224))
            img = img / 255.0  # Normalize
            X.append(img)
            y.append(label)
        elif file.endswith(('.mp4', '.avi', '.mov')):  # Process videos
            cap = cv2.VideoCapture(os.path.join(label_folder, file))
            ret, frame = cap.read()
            if ret:
                frame = cv2.resize(frame, (224, 224))
                frame = frame / 255.0  # Normalize
                X.append(frame)
                y.append(label)
            cap.release()

# Convert to NumPy arrays
X = np.array(X)
y = np.array(y)

# Encode labels numerically
y = encoder.transform(y)
print("âœ… Unique Labels in Dataset:", np.unique(y))

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Data Augmentation (Only during training)
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)
datagen.fit(X_train)

# Load Pretrained VGG16 Model
base_model = VGG16(weights="imagenet", include_top=False, input_shape=(224, 224, 3))
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.3)(x)
output_layer = Dense(len(all_labels), activation='softmax')(x)  # Adjust for the number of classes

# Define Model
model = Model(inputs=base_model.input, outputs=output_layer)

# Freeze base model layers
for layer in base_model.layers:
    layer.trainable = False

# Compile Model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train Model
history = model.fit(datagen.flow(X_train, y_train, batch_size=32), epochs=10, validation_data=(X_test, y_test))

# Evaluate Model
y_pred = np.argmax(model.predict(X_test), axis=1)
print("âœ… Classification Report:\n", classification_report(y_test, y_pred, target_names=encoder.classes_))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(12, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.classes_, yticklabels=encoder.classes_)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

pip install gTTS

from gtts import gTTS
import os

def predict_sign(image_path):
    img = cv2.imread(image_path)
    img = cv2.resize(img, (224, 224))
    img = img / 255.0
    img = np.expand_dims(img, axis=0)

    pred = np.argmax(model.predict(img))
    predicted_label = encoder.inverse_transform([pred])[0]

    print(f"ðŸ– Predicted Sign: {predicted_label}")
    return predicted_label

# Test Prediction
user_input_img = "/content/WhatsApp Image 2025-02-04 at 2.09.51 PM (1).jpeg"
predict_sign(user_input_img)

"""# *ResNet50 Model:*"""

import os
import cv2
import numpy as np
import mediapipe as mp
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Model
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# Paths
dataset_path = "/content/drive/MyDrive/FINAL ISL/isl"

# MediaPipe Hand Detection
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)

# Collect all available labels
all_labels = sorted([label for label in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, label))])
print("âœ… Available Classes:", all_labels)

# Encode labels
encoder = LabelEncoder()
encoder.fit(all_labels)  # Learn all labels

# Load dataset
X, y = [], []
for label in all_labels:
    label_folder = os.path.join(dataset_path, label)
    for file in os.listdir(label_folder):
        if file.endswith(('.jpg', '.png', '.jpeg')):  # Process images
            img_path = os.path.join(label_folder, file)
            img = cv2.imread(img_path)
            img = cv2.resize(img, (224, 224))
            img = img / 255.0  # Normalize
            X.append(img)
            y.append(label)
        elif file.endswith(('.mp4', '.avi', '.mov')):  # Process videos
            cap = cv2.VideoCapture(os.path.join(label_folder, file))
            ret, frame = cap.read()
            if ret:
                frame = cv2.resize(frame, (224, 224))
                frame = frame / 255.0  # Normalize
                X.append(frame)
                y.append(label)
            cap.release()

# Convert to NumPy arrays
X = np.array(X)
y = np.array(y)

# Encode labels numerically
y = encoder.transform(y)
print("âœ… Unique Labels in Dataset:", np.unique(y))

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Data Augmentation (Only during training)
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)
datagen.fit(X_train)

# Load Pretrained ResNet50 Model
base_model = ResNet50(weights="imagenet", include_top=False, input_shape=(224, 224, 3))
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.3)(x)
output_layer = Dense(len(all_labels), activation='softmax')(x)  # Adjust for the number of classes

# Define Model
model = Model(inputs=base_model.input, outputs=output_layer)

# Freeze base model layers
for layer in base_model.layers:
    layer.trainable = False

# Compile Model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train Model
history = model.fit(datagen.flow(X_train, y_train, batch_size=32), epochs=20, validation_data=(X_test, y_test))

# Evaluate Model
y_pred = np.argmax(model.predict(X_test), axis=1)
print("âœ… Classification Report:\n", classification_report(y_test, y_pred, target_names=encoder.classes_))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(12, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.classes_, yticklabels=encoder.classes_)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

from gtts import gTTS
import os

def predict_sign(image_path):
    img = cv2.imread(image_path)
    img = cv2.resize(img, (224, 224))
    img = img / 255.0
    img = np.expand_dims(img, axis=0)

    pred = np.argmax(model.predict(img))
    predicted_label = encoder.inverse_transform([pred])[0]

    print(f"ðŸ– Predicted Sign: {predicted_label}")
    return predicted_label

# Test Prediction
user_input_img = "/content/WhatsApp Image 2025-02-04 at 2.09.51 PM (1).jpeg"
predict_sign(user_input_img)

"""# *MobileNetV2 Model:*"""

import os
import cv2
import numpy as np
import mediapipe as mp
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Model
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D, BatchNormalization
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import ReduceLROnPlateau
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# Paths
dataset_path = "/content/drive/MyDrive/FINAL ISL/isl"

# Collect all available labels
all_labels = sorted([label for label in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, label))])
print("âœ… Available Classes:", all_labels)

# Encode labels
encoder = LabelEncoder()
encoder.fit(all_labels)  # Learn all labels

# Load dataset
X, y = [], []
for label in all_labels:
    label_folder = os.path.join(dataset_path, label)
    for file in os.listdir(label_folder):
        if file.endswith(('.jpg', '.png', '.jpeg')):  # Process images
            img_path = os.path.join(label_folder, file)
            img = cv2.imread(img_path)
            img = cv2.resize(img, (224, 224))
            img = img / 255.0  # Normalize
            X.append(img)
            y.append(label)
        elif file.endswith(('.mp4', '.avi', '.mov')):  # Process videos
            cap = cv2.VideoCapture(os.path.join(label_folder, file))
            ret, frame = cap.read()
            if ret:
                frame = cv2.resize(frame, (224, 224))
                frame = frame / 255.0  # Normalize
                X.append(frame)
                y.append(label)
            cap.release()

# Convert to NumPy arrays
X = np.array(X)
y = np.array(y)

# Encode labels numerically
y = encoder.transform(y)
print("âœ… Unique Labels in Dataset:", np.unique(y))

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Data Augmentation (Only during training)
datagen = ImageDataGenerator(
    rotation_range=30,  # Increased variation
    width_shift_range=0.3,
    height_shift_range=0.3,
    shear_range=0.2,
    zoom_range=0.3,
    horizontal_flip=True
)
datagen.fit(X_train)

# Load Pretrained MobileNetV2 Model
base_model = MobileNetV2(weights="imagenet", include_top=False, input_shape=(224, 224, 3))

# Custom Fully Connected Layers with Regularization
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(x)  # L2 Regularization
x = BatchNormalization()(x)  # Batch Normalization
x = Dropout(0.5)(x)  # Increased Dropout

x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)

output_layer = Dense(len(all_labels), activation='softmax')(x)  # Adjust for 36 classes

# Define Model
model = Model(inputs=base_model.input, outputs=output_layer)

# Freeze base model layers
for layer in base_model.layers:
    layer.trainable = False

# Compile Model with Regularization
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Learning Rate Scheduler
lr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, verbose=1)

# Train Model with Callbacks
history = model.fit(datagen.flow(X_train, y_train, batch_size=32), epochs=20, validation_data=(X_test, y_test), callbacks=[lr_reduction])

# Evaluate Model
y_pred = np.argmax(model.predict(X_test), axis=1)
print("âœ… Classification Report:\n", classification_report(y_test, y_pred, target_names=encoder.classes_))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(12, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.classes_, yticklabels=encoder.classes_)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Model
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D, BatchNormalization
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import ReduceLROnPlateau
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# Paths
dataset_path = "/content/drive/MyDrive/FINAL ISL/isl"

# Collect all available labels
all_labels = sorted([label for label in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, label))])
print("âœ… Available Classes:", all_labels)

# Encode labels
encoder = LabelEncoder()
encoder.fit(all_labels)  # Learn all labels

# Load dataset
X, y = [], []
num_frames = 10  # Number of frames to extract per video

for label in all_labels:
    label_folder = os.path.join(dataset_path, label)
    for file in os.listdir(label_folder):
        file_path = os.path.join(label_folder, file)

        if file.endswith(('.jpg', '.png', '.jpeg')):  # Process images
            img = cv2.imread(file_path)
            img = cv2.resize(img, (224, 224))
            img = img / 255.0  # Normalize
            X.append(img)
            y.append(label)

        elif file.endswith('.mp4'):  # Process videos
            cap = cv2.VideoCapture(file_path)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

            if frame_count >= num_frames:
                frame_indices = np.linspace(0, frame_count - 1, num_frames, dtype=int)
            else:
                frame_indices = np.linspace(0, frame_count - 1, frame_count, dtype=int)  # Extract available frames if less than num_frames

            extracted_frames = 0
            for i in range(frame_count):
                ret, frame = cap.read()
                if not ret:
                    break  # Stop if no more frames

                if i in frame_indices:
                    frame = cv2.resize(frame, (224, 224))
                    frame = frame / 255.0  # Normalize
                    X.append(frame)
                    y.append(label)
                    extracted_frames += 1

            cap.release()

# Convert to NumPy arrays
X = np.array(X)
y = np.array(y)

# Encode labels numerically
y = encoder.transform(y)
print("âœ… Unique Labels in Dataset:", np.unique(y))

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Data Augmentation (Only during training)
datagen = ImageDataGenerator(
    rotation_range=30,
    width_shift_range=0.3,
    height_shift_range=0.3,
    shear_range=0.2,
    zoom_range=0.3,
    horizontal_flip=True
)
datagen.fit(X_train)

# Load Pretrained MobileNetV2 Model
base_model = MobileNetV2(weights="imagenet", include_top=False, input_shape=(224, 224, 3))

# Custom Fully Connected Layers with Regularization
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)

x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)

output_layer = Dense(len(all_labels), activation='softmax')(x)  # Adjust for 36 classes

# Define Model
model = Model(inputs=base_model.input, outputs=output_layer)

# Freeze base model layers
for layer in base_model.layers:
    layer.trainable = False

# Compile Model with Regularization
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Learning Rate Scheduler
lr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, verbose=1)

# Train Model with Callbacks
history = model.fit(datagen.flow(X_train, y_train, batch_size=32), epochs=30, validation_data=(X_test, y_test), callbacks=[lr_reduction])

# Evaluate Model
y_pred = np.argmax(model.predict(X_test), axis=1)
print("âœ… Classification Report:\n", classification_report(y_test, y_pred, target_names=encoder.classes_))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(12, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.classes_, yticklabels=encoder.classes_)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# Save the trained model
model.save('/content/drive/MyDrive/FINAL ISL/isl_model.h5')
print("âœ… Model saved successfully!")

model.summary()

pip install gTTS

import os
import cv2
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from sklearn.preprocessing import LabelEncoder
from gtts import gTTS
from IPython.display import Audio, display
import matplotlib.pyplot as plt

# Load trained model
model_path = "/content/drive/MyDrive/FINAL ISL/isl_model.h5"
model = load_model(model_path)

# Load label encoder
dataset_path = "/content/drive/MyDrive/FINAL ISL/isl"
all_labels = sorted([label for label in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, label))])
encoder = LabelEncoder()
encoder.fit(all_labels)

# Function to process and predict sign from an image
def predict_sign(image_path):
    img = cv2.imread(image_path)
    if img is None:
        print("Unable to read the image file.")
        return None

    # Preprocess Image
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB for display
    img_resized = cv2.resize(img, (224, 224))
    img_normalized = img_resized / 255.0  # Normalize
    img_expanded = np.expand_dims(img_normalized, axis=0)  # Add batch dimension

    # Display Image
    plt.imshow(img_rgb)
    plt.axis("off")
    plt.title("Input Image")
    plt.show()

    # Predict Sign
    pred = np.argmax(model.predict(img_expanded))
    predicted_label = encoder.inverse_transform([pred])[0]
    print(f" Predicted Sign: {predicted_label}")

    # Convert prediction to speech
    tts = gTTS(text=predicted_label, lang='en')
    audio_path = "output.mp3"
    tts.save(audio_path)

    # Play audio
    display(Audio(audio_path, autoplay=True))

    return predicted_label

# Loop until user provides a valid image path
while True:
    user_input_img = input("Enter the path of the image file: ").strip()

    if not os.path.exists(user_input_img):
        print("File does not exist. Please enter a valid path.")
        continue  # Ask again

    if not user_input_img.lower().endswith(('.jpg', '.png', '.jpeg')):
        print("Invalid file format. Please provide an image (.jpg, .png, .jpeg).")
        continue  # Ask again

    # If input is valid, process and predict
    predict_sign(user_input_img)
    break  # Exit loop after successful prediction

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from gtts import gTTS
from IPython.display import Audio, display

# Function to process and predict sign from a video
def predict_sign_from_video(video_path, frame_skip=10):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("Unable to open the video file.")
        return None

    frame_count = 0
    predictions = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Rotate if necessary
        if frame.shape[0] > frame.shape[1]:
            frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)

        # Process every nth frame (controlled by frame_skip)
        if frame_count % frame_skip == 0:
            img_resized = cv2.resize(frame, (224, 224))
            img_norm = img_resized / 255.0  # Normalize
            img_input = np.expand_dims(img_norm, axis=0)  # Add batch dimension

            # Predict Sign
            pred = np.argmax(model.predict(img_input))
            predicted_label = encoder.inverse_transform([pred])[0]
            predictions.append(predicted_label)

            # Display frame with prediction
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            plt.imshow(frame_rgb)
            plt.axis("off")
            plt.title(f"Predicted Sign: {predicted_label}")
            plt.show()

        frame_count += 1

    cap.release()

    # Determine final prediction based on most frequent label
    if predictions:
        final_prediction = max(set(predictions), key=predictions.count)
        print(f"\nFinal Predicted Sign from Video: {final_prediction}")

        # Convert prediction to speech
        tts = gTTS(final_prediction, lang='en')
        audio_path = "output.mp3"
        tts.save(audio_path)

        # Play audio
        display(Audio(audio_path, autoplay=True))

        return final_prediction
    else:
        print("No frames were processed. Please try another video.")
        return None

while True:
    video_path = input("Enter the path of the video file: ").strip()

    if not os.path.exists(video_path):
        print("File does not exist. Please enter a valid path.")
        continue

    if not video_path.lower().endswith(('.mp4', '.avi', '.mov')):
        print("Invalid file format. Please provide a video (.mp4, .avi, .mov).")
        continue  # Ask again

    # If input is valid, process the video
    predict_sign_from_video(video_path)
    break  # Exit loop after successful prediction

pip install gtts

from google.colab import drive
drive.mount('/content/drive')

